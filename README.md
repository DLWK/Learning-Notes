NB: Time below means when I "studied", not when it came.

注意：下列时间均为我“学习”的时间，而非它本身的时间。

## 2016-10

#### 2016-10-17
* WaveNet: A Generative Model for Raw Audio ([arXiv](https://arxiv.org/abs/1609.03499)) ([blog](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)) ([code](https://github.com/usernaamee/keras-wavenet))

  The repo of code is a very simple version to read.

* CS224d ([Stanford](http://cs224d.stanford.edu/syllabus.html)): Lecture 3

### 2016-10-10 Week
* [Neural Style and Deep Dream](notes/neural-style-deep-dream.md)
  * A Neural Algorithm of Artistic Style ([arXiv](https://arxiv.org/abs/1508.06576))
  * Keras Deep Dream ([code](https://github.com/fchollet/keras/blob/master/examples/deep_dream.py))
  * Keras Neural Style ([code](https://github.com/fchollet/keras/blob/master/examples/neural_style_transfer.py))
  * CS231n: Lecture 9 ([slide](http://vision.stanford.edu/teaching/cs231n/slides/winter1516_lecture9.pdf))

* [REVISIT] Understanding LSTM Networks ([blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)) ([code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/6_lstm.ipynb))

  The code from Udacity Deep Learning course is exactly the same as described in the blog.

* RNNs in TensorFlow, a Practical Guide and Undocumented Features ([blog](http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/))
* [REVISIT] CS224d ([Stanford](http://cs224d.stanford.edu/syllabus.html)): Lecture 1 - 2
* CS231n ([Stanford](http://vision.stanford.edu/teaching/cs231n/syllabus.html)): Lecture 1

### 2016-10-03 Week
* Building Autoencoders in Keras ([blog](https://blog.keras.io/building-autoencoders-in-keras.html))
* Deep Learning for ChatBots
  * Part 1 - Introducation ([blog](http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/))
  * Part 2 – Implementing a Retrieval-based Model in TensorFlow ([blog](http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/))
* Neural Networks for Machine Learning by Geoffrey Hinton ([Coursera](https://www.coursera.org/learn/neural-networks/)): Week 1 - 3

## 2016-09
* [Spatial Transformer Networks](notes\spatial-transformer-network.md) ([arXiv](https://arxiv.org/abs/1506.02025)) ([blog](http://torch.ch/blog/2015/09/07/spatial_transformers.html)) ([code](https://github.com/tensorflow/models/tree/master/transformer))
* Attention and Memory in Deep Learning and NLP ([blog](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/))
* Deep Residual Learning for Image Recognition ([arXiv](http://arxiv.org/abs/1512.03385))
* Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning ([arXiv](https://arxiv.org/abs/1602.07261))
* Bilinear CNN Models for Fine-grained Visual Recognition ([arXiv](https://arxiv.org/abs/1504.07889))
